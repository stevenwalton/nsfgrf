% --------------- 12 POINT FONT -------------------------------
\documentclass[12pt]{article}
% --------------- 10 POINT FONT FOR CAPTIONS ------------------
\usepackage[font=footnotesize]{caption}
% --------------- NY TIMES FONT -------------------------------
\usepackage{times}
% --------------- 1 INCH MARGINS ------------------------------
\usepackage[margin=1in]{geometry}
% --------------- LINE SPACING --------------------------------
\usepackage{setspace}
\singlespacing
%\doublespacing
% --------------- SMALL SECTION TITLES ------------------------
\usepackage[tiny,compact]{titlesec}
% --------------- MATH PACKAGES -------------------------------
\usepackage{amsmath,amsthm,amssymb}
\setlength{\parindent}{0pt}
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}
\begin{document}
% --------------- TITLE AND NAME ------------------------------
\begin{center}
\underline{\bf Graduate Research Plan Statement}\\
{\bf Time-Series Interpolation of Scientific Data via Machine Learning} \\
\end{center}
% --------------- CONTENT -------------------------------------


\textbf{Background:}
%
Big Data is a major challenge for computational simulations 
in High Performance Computing (HPC).
%
Traditionally, these simulations save their state at
regular intervals to disk, which enables post hoc analysis
and visualization after the simulation has run.
%
However, while disk capacity and bandwidth are increasing on each
new generation of supercomputer, they are increasing much more slowly
than the ability to compute data.
%
For example, over the last decade, the Texas Advanced Computing Center (TACC) 
supercomputers have increased disk bandwidth by approximately a factor of ten, 
while compute power (i.e., the ability to compute data) has increased by a 
factor of one hundred.
%
%
As a result, the traditional method for post hoc analysis and visualization
is rapidly falling out of favor. Instead, simulations are increasingly
incorporating in situ processing,
i.e., analyzing and performing
visualization tasks while the data is being generated. 
%
% Don't like "with in situ" too close to "within situ"
%With in situ methods, simulation data can be significantly reduced, and then
%stored in a reduced form.
In situ methods allow analysis to be performed on a subset of the 
simulation data and processed results are then stored in a reduced form. Further
analysis can then be explored in a post hoc manner. 
%
However, this can have major
drawbacks --- scientists cannot re-analyze or come back to parts of
simulations after they have thought more deeply about problems or gained insights
from other work. 
%
In essence, scientists have to hope that they got the analysis
correct the first time, otherwise have to go back and re-perform part or all of
the simulation again. 
%
To solve this problem, we need to develop in situ techniques that maximize the information 
extracted while the simulation is running.
%
This is an open research problem.

Fortunately, the explosion in machine learning research has generated many
techniques that can potentially help the computational simulation
community. 
%
While several relevant works have shown that machine learning can play a role in scientific
visualization~\cite{Berg,He,super}, additional research is needed to explore how
machine learning can be used to address the problem of maximizing the in situ 
extraction of information.
%Techniques like super resolution~\cite{super} can be used to
%compress images, greatly reducing storage space. Machine learning algorithms are
%capable of performing scientific computations~\cite{Berg}, as well as explore
%the parameter space of scientific simulations~\cite{He}. 
%These techniques 
%allow for post hoc exploration of varying parameters without performing new
%simulations.


\textbf{Research Question:} Machine learning's role for in situ visualization is
still an open question.
%
The focus of my proposed research is asking how machine learning might be used
to interpolate time-series scientific data. This question leads to asking many
sub questions.  One such sub question is, how can simulation data be encoded in
a manner useful to machine learning algorithms?
%
This involves determining what features are relevant to simulation data, and how
to compress or encode the relevant information. Additionally, can the relevant
features for HPC simulation data be stored on modern GPUs? And if so, how?
%
What methods might be used to perform interpolation on highly irregular data?
Can machine learning algorithms learn enough physics to make accurate
interpolations with meaningful temporal differences? 
%
Additionally, which machine learning techniques execute quickly enough to be
used in situ? 
%
These questions will need to be answered to determine what role machine
learning can play for in situ visualization.


\textbf{Work Plan:}
My work plan for this fellowship addresses my research questions.
To achieve this I will
break down the project into five tasks that will be spread out over a three
year plan. The tasks are to: T1) find feature reduction techniques, T2) research
compression and encoding algorithms to fit data into GPU memory, T3) explore
techniques for time-series interpolation on scientific data, T4) study the
generalization and limitations of the developed algorithms, T5) utilize online
algorithms so the learning can be done in situ. I describe my specific plans for
these tasks in the remainder of this section.

%The first task will be to analyze and develop methods for feature
%reduction so that data can fit onto GPU memory. The second task will be to
%research algorithms that demonstrate compression and encoding techniques, on
%scientific data, that improves on current methods. Third, I will explore
%techniques to perform basic time-series interpolation of scientific data.
%Fourth, I will research the ability to generalize these techniques and determine
%the limitations to each, and where they can best be applied. Fifth, I will focus
%on converting these to online algorithms, so that training and fine tuning can
%be done in situ.

T1, the motivation for feature reduction techniques for
scientific data is that HPC simulations can produce petabytes of data a
day, which is much larger than data sets conventionally used in machine learning
algorithms. Current GPU architectures, which are essential for machine learning,
are limited in memory. I will explore established techniques, such as
t-SNE and Principle Component Analysis, to reduce the feature space and
determine which features are necessary.

The motivation for T2, researching compression and encoding techniques to  
fit the required feature space and architecture into GPU memory, is to make
simulation data as small as possible to reduce training time and 
computational load. One of the breakthroughs in modern machine learning is using
it for compression. It has been shown that machine learning can be used to
perform complex encodings of data that can be used to regenerate the original
set. This type of encoding will be important for several reasons. A
good encoding can reduce memory burden and free up resources for
competing computations. This can also enable more checkpointing for the
simulations, where a checkpoint can allow for a simulation to be run from that
point instead of starting again from the beginning. With the completion of this
milestone, scientists will be able to more easily restart simulations and be able
to save more data out for post hoc analysis.

The goal for T3, researching techniques for time series interpolation methods,
is to develop architectures that can predict the underlying physics and perform
interpolations over large time steps. Some of the newest machine learning
techniques provide methods that imply this possibility. Modern architectures,
such as Recurrent Neural Networks and Transformers, demonstrate the
ability for the algorithms to become contextually aware. An important aspect of
interpolating scientific data is to understand how parts of the data move within
a given system. Conventional methods use momentum of data points to determine
how different subsections of data evolve compared to others. Modern machine
learning architectures present the possibility for similar contextual knowledge
to be learned by the algorithm.
%These architectures have provided great leaps forward in Natural Language
%Processing with the ability to construct large scale text that is nearly
%indistinguishable from human writing~\cite{radford2019language}. This same
%contextual awareness can theoretically be applied to the process of scientific
%analysis.  . These architectures provide a necessary component that helps
%enable algorithms to understand the structure of time within scientific
%computing. 

T4, researching the generalizability of these
interpolation algorithms, generalization is one of the most difficult tasks in
machine learning. With any algorithmic development it is important to understand
its applications and limitations. A deep analysis will need to be performed to
determine the extent of generalization and how much fine tuning will need to be
done. 

T5, convert our algorithms to online algorithms so
that fine tuning can happen in situ, which adds flexibility for scientists
and decrease the computational demands of the algorithms. 


\textbf{Intellectual Merit:}
This proposal aims to apply machine learning to scientific data sets for
computer simulations. It endeavors to do fundamental research on how large data
sets can be mapped into machine learning frameworks and how these frameworks can
be used to solve a specific problem -- time series interpolation. 
%Success of this research would lead to a reduction in the search space for
%simulation scientists. It would also lead to a reduction of data storage and
%potentially lead to greater capabilities in both in situ and post hoc analysis. 
%Success would result in less computational resources required as well as
%decrease the time that scientists need to perform their research. 
%
\\\textbf{Broader Impacts:} 
Success of this research would result in a large reduction in memory consumption
of HPC applications while also allowing for better understanding of data. HPC is
being used to tackle some of the toughest scientific challenges facing humanity,
namely climate research, clean energy, and medical research. Enabling this
science to be done more efficiently and faster directly results in a higher
quality for people living on the planet. Through NSF this research can be public
and help scientists everywhere have better tools when facing the most
challenging problems.

% --------------- WORKS CITED (10pt FONT) ---------------------

%\footnotesize
\scriptsize
\bibliographystyle{acm}
\bibliography{Research}
%\begin{thebibliography}{aa}
%
%%\bibitem{amau} Maurer, Andrew B. \emph{\LaTeX \ Template for the National Science Foundation's Graduate Research fellowship Program}.
%
%\end{thebibliography}
\end{document}

