% --------------- 12 POINT FONT -------------------------------
\documentclass[12pt]{article}
% --------------- 10 POINT FONT FOR CAPTIONS ------------------
\usepackage[font=footnotesize]{caption}
% --------------- NY TIMES FONT -------------------------------
\usepackage{times}
% --------------- 1 INCH MARGINS ------------------------------
\usepackage[margin=1in]{geometry}
% --------------- LINE SPACING --------------------------------
\usepackage{setspace}
\singlespacing
%\doublespacing
% --------------- SMALL SECTION TITLES ------------------------
\usepackage[tiny,compact]{titlesec}
% --------------- MATH PACKAGES -------------------------------
\usepackage{amsmath,amsthm,amssymb}
\setlength{\parindent}{0pt}
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}
\begin{document}
% --------------- TITLE AND NAME ------------------------------
\begin{center}
\underline{\bf Graduate Research Plan Statement}\\
{\bf Time-Series Interpolation of Scientific Data via Machine Learning} \\
\end{center}
% --------------- CONTENT -------------------------------------


\textbf{Background:}
%
Big Data is a major challenge for computational simulations 
in High Performance Computing (HPC).
%
Traditionally, these simulations save their state at
regular intervals to disk, which enables post hoc analysis
and visualization after the simulation has run.
%
However, while disk capacity and bandwidth are increasing on each
new generation of supercomputer, they are increasing much more slowly
than the ability to compute data.
%
For example, over the last decade, the Texas Advanced Computing Center (TACC) 
supercomputers have increased disk bandwidth by approximately a factor of ten, 
while compute power (i.e., the ability to compute data) has increased by a 
factor of one hundred.
%
%
As a result, the traditional method for post hoc analysis and visualization
is rapidly falling out of favor. Instead, simulations are increasingly
incorporating in situ processing,
i.e., analyzing and performing
visualization tasks while the data is being generated. 
%
% Don't like "with in situ" too close to "within situ"
%With in situ methods, simulation data can be significantly reduced, and then
%stored in a reduced form.
In situ methods allow analysis to be performed on a subset of the 
simulation data and processed results are then stored in a reduced form. Further
analysis can then be explored in a post hoc manner. 
%
However, this can have major
drawbacks --- scientists cannot re-analyze or come back to parts of
simulations after they have thought more deeply about problems or gained insights
from other work. 
%
In essence, scientists have to hope that they got the analysis
correct the first time, otherwise have to go back and re-perform part or all of
the simulation again. 
%
To solve this problem, we need to develop in situ techniques that maximize the information 
extracted while the simulation is running.
%
This is an open research problem.

Fortunately, the explosion in machine learning research has generated many
techniques that can potentially help the computational simulation
community. 
%
While several relevant works have shown that machine learning can play a role in scientific
visualization~\cite{Berg,He,super}, additional research is needed to explore how
machine learning can be used to address the problem of maximizing the in situ 
extraction of information.
%Techniques like super resolution~\cite{super} can be used to
%compress images, greatly reducing storage space. Machine learning algorithms are
%capable of performing scientific computations~\cite{Berg}, as well as explore
%the parameter space of scientific simulations~\cite{He}. 
%These techniques 
%allow for post hoc exploration of varying parameters without performing new
%simulations.


\textbf{Research Question:} Machine learning's role for in situ visualization is
still an open question.
%
The focus of my proposed research is asking how machine learning might be used
to interpolate time-series scientific data. This question leads to asking many
sub questions.  One such sub question is, how can simulation data be encoded in
a manner useful to machine learning algorithms?
%
This involves determining what features are relevant to simulation data, and how
to compress or encode the relevant information. Additionally, can the relevant
features for HPC simulation data be stored on modern GPUs? And if so, how?
%
What methods might be used to perform interpolation on highly irregular data?
Can machine learning algorithms learn enough physics to make accurate
interpolations with meaningful temporal differences? 
%
Additionally, which machine learning techniques execute quickly enough to be
used in situ? 
%
These questions will need to be answered to determine what role machine
learning can play for in situ visualization.


\textbf{Work Plan:}
My work plan for this fellowship addresses my research questions.
To achieve this I will
break down the project into five tasks that will be spread out over a three
years plan. The tasks are to: 1) find feature reduction techniques, 2) research
compression and encoding algorithms to fit data into GPU memory, 3) explore
techniques for time-series interpolation on scientific data, 4) study the
generalization and limitations of the developed algorithms, 5) utilize online
algorithms so the learning can be done in situ. I describe my specific plans for
these tasks in the remainder of this section.

%The first task will be to analyze and develop methods for feature
%reduction so that data can fit onto GPU memory. The second task will be to
%research algorithms that demonstrate compression and encoding techniques, on
%scientific data, that improves on current methods. Third, I will explore
%techniques to perform basic time-series interpolation of scientific data.
%Fourth, I will research the ability to generalize these techniques and determine
%the limitations to each, and where they can best be applied. Fifth, I will focus
%on converting these to online algorithms, so that training and fine tuning can
%be done in situ.

 The first task is to research feature reduction techniques for
scientific data. In an HPC setting, simulations can produce petabytes of data a
day, which is much larger than data sets conventionally used in machine learning
algorithms. Current GPU architectures cannot store this much data but are
essential for machine learning. I will explore established techniques, such as
t-SNE and Principle Component Analysis, to reduce the feature space and
determine which features are necessary.

 The second task is to research compression and encoding techniques to
fit the required features space and architecture into GPU memory. Making
simulation data as small as possible reduces training time and reduces the
computational load. One of the breakthroughs in modern machine learning is using
it for compression. It has been shown that machine learning can be used to
perform complex encodings of data that can be used to regenerate the original
set. This type of encoding will be important for several reasons. Once we have a
good encoding we can reduce our memory burden and free up resources for
competing computations. This can also enable more checkpointing for the
simulations; where a checkpoint can allow for a simulation to be run from that
point instead of starting again from the beginning. With the completion of this
milestone scientists will be able to more easily restart simulations and be able
to save more data out for post hoc analysis.

 The third task is to research techniques for time series interpolation
methods. The goal is to develop architectures that can predict 
the underlying physics and perform interpolations over large time-steps. Some
of the newest machine learning techniques provide methods that imply this
possible. Architectures like Recurrent Neural Networks (RNNs), Transformers, and
Attention demonstrate the ability for the algorithms to become contextually
aware. An important aspect of interpolating scientific data is to understand how
parts of the data move within a given system. Conventional methods use momentum
of data points to determine how different subsections of data evolve compared to
others. These architectures have provided great leaps forward in Natural
Language Processing with the ability to construct large scale text that is
nearly indistinguishable from human writing~\cite{radford2019language}. This
same contextual awareness can theoretically be applied to the language of
scientific analysis. These architectures provide a necessary component that
helps enable algorithms to understand the structure of time within scientific
computing. 

 The fourth task to to research the generalizability of these
interpolation algorithms. Generalization is one of the most difficult tasks in
machine learning. With any algorithmic development it is important to understand
its applications and limitations. A deep analysis will need to be performed to
determine the extent of generalization and how much fine tuning will need to be
done. 

 The fifth task is to convert these algorithms to online algorithms so
that fine tuning can happen in situ. This will add flexibility for scientists
and decrease the computational demands of the algorithms. 


\textbf{Intellectual Merit:}
Success of this research would lead to a reduction in the search space for
simulation scientists. It would also lead to a reduction of data storage and
potentially lead to greater capabilities in both in situ and post hoc analysis. 
Success would result in less computational resources required as well as
decrease the time that scientists need to perform their research. 
%
\\\textbf{Broader Impacts:} 
Success of this research would result in a large reduction in memory consumption
of HPC applications while also allowing for better understanding of data. HPC is
being used to tackle some of the toughest scientific challenges facing humanity,
namely climate research, clean energy, and medical research. Enabling this
science to be done more efficiently and faster directly results in a higher
quality for people living on the planet. Through NSF this research can be public
and help scientists everywhere have better tools when facing the most
challenging problems.

% --------------- WORKS CITED (10pt FONT) ---------------------

%\footnotesize
\scriptsize
\bibliographystyle{acm}
\bibliography{Research}
%\begin{thebibliography}{aa}
%
%%\bibitem{amau} Maurer, Andrew B. \emph{\LaTeX \ Template for the National Science Foundation's Graduate Research fellowship Program}.
%
%\end{thebibliography}
\end{document}

